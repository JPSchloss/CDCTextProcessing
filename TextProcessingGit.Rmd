---
title: "TextProcessing"
output: html_document
---
##### Prepared By: Jonathan Schlosser
##### Website: JonathanSchlosser.com 

## Webscraping

Webscraping is the process of collecting data from a web page, extracting necessary information from the webpage, and converting it into a usable format. Here we are using the rvest package to go to a Wikipedia page, and we are extracting the paragraph text. Often, to successfully webscrape, you will need some basic knowledge of HTML. Here, we are collecting the web information labeled with a \<p\> tag and we are extracting the text between \<p\> and \</p\>. The \<p\> tag is quite common, especially when trying to collect text, as it represents the HTML paragraph element. 

```{r message=FALSE}
# Loading in the library
#install.packages('rvest')
library(rvest)

# Identifying our URL of interest
url <- "https://en.wikipedia.org/wiki/University_of_North_Carolina_at_Chapel_Hill"

# Retreiving the webpage
web_page <- read_html(url)

# Identifying the HTML nodes of interest
web_nodes <- html_nodes(web_page, 'p')

# Extracting the text from those HTML nodes
web_text <- html_text(web_nodes)

# Displaying one instance
web_text[2]

```

## String Basics

There are a ton of different ways to work with strings in R. You could use base commands, you could use the GREP functions, you could use tidytext approaches, or you could use a combination of all three depending on your task. Here we are using the 'stringr' package, which is commonly used in the tidy universe. Tidy approaches are comonly associated with R and are often encouraged because of the principles upon which they are built. But, this does not mean that they are superior, they're just different. 

Below, some common string manipulation tasks are outlined. These are mostly used in combination with other tasks or when prepping/wrangling data. 

```{r message=FALSE}
# Loading in the library.
#install.packages('stringr')
library(stringr)
```

#### String Length
```{r}
# Identifying the length of a string.
str_length(web_text[2])
```

#### Detecting Strings
```{r}
# Detecting whether or not one instance of "Chapel Hill" is in the string. 
str_detect(web_text[2], 'Chapel Hill')
```

#### Counting Strings
```{r}
# Counting the instances of "Chapel Hill" is in the string. 
str_count(web_text[2], 'Chapel Hill')
```

#### Subsetting Strings
```{r}
# Subsetting all of the strings that contain 'Chapel Hill".
  # This perserves the entire string. 
chapel <- str_subset(web_text, 'Chapel Hill')

head(chapel, 3)
```

#### Extracting Strings
```{r}
# Extracting the first instance of the pattern within each string. Replaces non-matches with NA values. 
chapel <- str_extract(web_text, 'Chapel Hill')
head(chapel, 5)
```

```{r}
# Extracting all instances of the pattern within each string.
chapel <- str_extract_all(web_text, 'Chapel Hill')
head(chapel, 5)
```

#### Replacing Strings
```{r}
# Replacing the first instance of the pattern within each string.
web_text[2]

chapel <- str_replace(web_text[2], 'Chapel Hill', 'A Cool Place')

chapel
```

```{r}
# Replacing sll instances of the pattern within each string.
web_text[2]

chapel <- str_replace_all(web_text[2], 'Chapel Hill', 'A Cool Place')

chapel
```

#### Converting String Case
```{r}
# Coverting the string to lowercase.
lower <- str_to_lower(web_text[2])
lower

# Coverting the vector to upper case.
upper <- str_to_upper(web_text)
head(upper, 3)

# Converting the vector to title case.
title <- str_to_title(upper)
head(title, 3)

```

## Initial Data Wrangling

Here we are just collecting some raw data, doing some initial cleaning, and shaping the data for our future analyses. We are collecting Grimms Fairy Tales from the Gutenberg Project. This step will change based on your initial data and how you choose to work with your data. 


```{r message=FALSE}
# Loading in the libraries
#install.packages('tidyverse')
library(tidyverse)
#install.packages('gutenbergr')
library(gutenbergr)

# Downloading Grimms Fairy Tales from Project Gutenberg (http://gutenberg.org/ebooks/2591)
grimms_raw <- gutenberg_download(2591)

# Cleaning, wrangling, and setting factors for the data. 
grimms <- grimms_raw[94:9172,] %>%
  mutate(tale = ifelse(str_detect(text, "[[:upper:]]{3,}$"),
                          text, NA)) %>%
  fill(tale) %>%
  mutate(tale = factor(tale, levels = unique(tale)))
  
head(grimms, 10)

```

## Text Processing

Below, we are doing a fairly significant step. We are tokenizing, converting the text to lowercase, removing punctuation, and removing whitspaces. The unnest_tokens function is doing the brunt of the work here. Note though, this step does not remove numbers. 

Also, we are removing stop words. Common words such as “the”, “and”, “for”, “is”, etc. can be considered “stop words,” as they do not convey valuable meaning and probably should be removed from the analyses. We are using the stop_words dictionary as part of the tidytext package, which uses a set list of common words within the English language. There are a few commonly used English stop word dictionaries within R and there are many dictionaries for languages outside of English. But, it is also common to add or remove words from a stop word dictionary, to build your own dictionary, or to have some other sort of additional set of terms that you'd like to remove. 

```{r message=FALSE}
#install.packages('tidytext')
library(tidytext)

# Unnesting tokens and removing stop words
tidy_grimms <- grimms %>%
    mutate(line = row_number()) %>%
    unnest_tokens(word, text) %>%
    anti_join(stop_words)

head(tidy_grimms)
```

If we wanted to stem the words in the code, we would use the SnowballC package and include a line like "mutate(WordStem = wordStem(word))". Stemming works to replace the words in our text with their most basic conjugate form. Stemming works to change a word such as "bringing" into its root form of "bring". This is a way of reducing our data set and loosening up the analysis abilities of the model. 

We could also filter terms based on their length here by incorporating a line of code like "filter(str_length(word) > 2)". This would also help to reduce our data. 

Overall, this is the step where we apply our decision points and clean the text data in the way that we need for our analyses. 


## Top Terms

Here we are just quickly counting up the number of times each term appears in the dataset and identifying the top ten. You could use this make a graph or to add to your analysis in actual practice, but this really is just a preliminary step and is a very common practice. 

```{r message=FALSE}
# Looking at at the initial term counts. 
tidy_grimms_count <- tidy_grimms %>%
    count(word, sort = TRUE)

head(tidy_grimms_count, 10)

```

## TF-IDF

TF-IDF (Term Frequency * Inverse Document Frequency) is a measure to determine the uniqueness of the terms within the dataset. Essentially, we are trying to find unusual words that might set one document apart from the others. This becomes farily important in more advanced forms of pattern recognition. The TF-IDF increases the more a term appears in a document but it is negatively impacted by the overall frequency of the term across all documents in the dataset

```{r message=FALSE}
# Caluclating the TF-IDF with the bind_tf_idf function. 
grimms_tf_idf <- tidy_grimms %>%
    count(tale, word, sort = TRUE) %>%
    bind_tf_idf(word, tale, n) %>%
    arrange(-tf_idf) %>%
    group_by(tale) %>%
    top_n(10) %>%
    ungroup

head(grimms_tf_idf)

# Plotting the above values. 
grimms_tf_idf %>%
    mutate(word = reorder_within(word, tf_idf, tale)) %>%
    filter(tale == c("THE GOLDEN BIRD", "HANS IN LUCK", "THE FROG-PRINCE")) %>%
    ggplot(aes(word, tf_idf, fill = tale)) +
    geom_col(alpha = 0.8, show.legend = FALSE) +
    facet_wrap(~ tale, scales = "free", ncol = 3) +
    scale_x_reordered() +
    coord_flip() +
    theme(strip.text=element_text(size=11)) +
    labs(x = NULL, y = "TF-IDF",
         title = "Highest TF-IDF Words in Grimms Fairy Tales")
```

######################
## Sentiment Analysis

Sentiment anlyses employ a sentiment dictionary that has values associated with terms. These dictionaries are created in a variey of ways, but often they have extensive research and validation behind them. 

Here we will be using a dictionary from the tidytext package. We will be using the 'bing' sentiment dictionary that includes sentiment words identified in online forums. Alternatives include 'afinn' which was developed based on a list of sentiment laden words that appeared in Twitter discussions of climate change, and 'nrc' which was created by having Amazon Mechanical Turk workers code the emotional valence of terms. 

The algorithms often produce similar results even though they are trained on different datasets. 

Also, the different dictionaries have different scales and different values. One may be more beneficial for your analyses than another, and its up to you to choose which is the most appropriate for your analyses. 

```{r message=FALSE}
# Loading in libraries. 
#install.packages('textdata')
library(textdata)

# Getting the sentiments to be used in the analysis
bing <- get_sentiments("bing") #Alternatives: "afinn" or "nrc" 

# Calculating the sentiments for three tales.
grimms_sentiment <- tidy_grimms %>%
  filter(tale == c("THE GOLDEN BIRD", "HANS IN LUCK", "THE FROG-PRINCE")) %>%
  inner_join(bing) %>%
  count(tale, index = line %/% 20, sentiment) %>%
  spread(sentiment, n, fill = 0) %>%
  mutate(sentiment = positive - negative)

# Plotting the above sentiments. 
ggplot(grimms_sentiment, aes(index, sentiment, fill = tale)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~tale, ncol = 3, scales = "free_x")

```

Here, we have taken 20 line chunks of each tale and have calculated the overall sentiment of that chunk. This helps us see hos the sentiment changes as the tale progresses. This is kind of an aribtrary analysis but it shows one of many ways in which these approaches can be applied. 

You can use sentiment analysis to identify the overall sentiment in different things, how they compare, how they evolve, etc. 

Below, we are identifying the terms that are the most frequent in both positive and negative respects. 

```{r message=FALSE}
# Identifying the top positive and negative terms in the tales. Included the option to filter the analyis to only certain tales.
sentiment_word_counts <- tidy_grimms %>%
  #filter(tale == c("THE GOLDEN BIRD", "HANS IN LUCK", "THE FROG-PRINCE")) %>%
  inner_join(get_sentiments("bing")) %>%
  count(word, sentiment, sort = TRUE)

head(sentiment_word_counts)

# Displaying the above counts. 
sentiment_word_counts %>%
  group_by(sentiment) %>%
  top_n(10) %>%
  ungroup() %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n, fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~sentiment, scales = "free_y") +
  labs(y = "Contribution to sentiment",
       x = NULL) +
  coord_flip()

```

#######################
## Topic Modeling

Topic modeling assumes that words carry different meanings based upon their co-occurrence with other terms. This approach groups words together to capture the meaning of the words based upon their context. Topic modeling is not the only method to do this as it is common in cluster analyses, latent semantic analyses, and other similar techniques. 

Topic models are mixture models; meaning that each document is assigned a probability of beloning to a latent theme or 'topic' rather than restricting document to be assigned to only one topic. Therefore, a document can belong to multiple topics and would have a quantified likelihood of being in the topic. 

Below, we first create our Document-Term Matrix. A Document-Term Matrix is a matrix where the rows represent terms and the columns represent documents. The value in each instance is the number of times the word appears in the document. Also, since we are using a tidytext approach, this approach to topic modeling requires a Document-Term Matrix. 

Also, we are using the stm package here. This is one of a few packages in R that do topic modeling. Here we are employing a structural topic model or stm. The stm package can also do other forms of topic modeling including Latent Dirchilet Allocation (LDA) models and random modeling. Another package that could be used is topicmodels, which is often used for LDA analyses. The stm package in R is quite robust though and is fairly simple to implement. 

STMs are similar to LDAs but STMs employ metadata about the documents in order to construct the model, which is often absent from LDA analyses. 

```{r message=FALSE}
# Loading in the libraries. 
#install.packages('quanteda')
library(quanteda)
#install.packages('stm')
library(stm)

# Creating a Document Feature Matrix (DFM)
grimms_dfm <- tidy_grimms %>%
    count(tale, word, sort = TRUE) %>%
    cast_dfm(tale, word, n)

```

Below, we are checking diagnostics for the model. This runs through a series of K values and calculates diagnostics for each iteration. These diagnostics are then used to help guide your K-value decision. This is a major advantage of the stm package in R. Other packages can caluclate these diagnostics but they take a bit more work. 

```{r include=FALSE}
# Converting the DFM for the STM model. 
grimms_stm = convert(grimms_dfm, to = 'stm')

# Testing to find the appropirate K values. 
Find_K_Values <- searchK(grimms_stm$documents, grimms_stm$vocab, K = c(5, 10, 15, 20, 25),
                         data = grimms_stm$meta, seed = 5609, init.type = "Spectral")
```
```{r eval=FALSE}
# Converting the DFM for the STM model. 
grimms_stm = convert(grimms_dfm, to = 'stm')

# Testing to find the appropirate K values. 
Find_K_Values <- searchK(grimms_stm$documents, grimms_stm$vocab, K = c(5, 10, 15, 20, 25),
                         data = grimms_stm$meta, seed = 5609, init.type = "Spectral")
```
```{r message=FALSE}
# Plotting the diagnostics. 
plot(Find_K_Values)

```

Here we can see the Held-Out Likelihood, the Residuals, the Semantic Coherence, and the Lower Bound values. When considering the best K value, according to statistical analyses, we want to minimize the Resduals and maximize the Held-Out Likelihood. We also want to consider the semantic coherence, but this sometimes doesnt accurately represent how a human would code the values. So for our model, were gonna go with 15 as our K-value. 

Now, this is not necessarily a better method than visual analysis. Often, the model may output incomprehnsible topics and further refinement is needed. There are also a number of other evaluation and validation techniques that could come into plat. Topic modeling here becomes almost like an art form. 

```{r message=FALSE}
# Running the topic model
topic_model <- stm(grimms_dfm, K = 15, 
                   verbose = FALSE, init.type = "Spectral", seed = 5609)

```

Now, lets identify which terms have the highest probability of being included in a topic. This helps us get a feel for what our topics actually represent. 

```{r message=FALSE}
# Using the tidy package to quickly and easily extract data from the model. 
grimms_beta <- tidy(topic_model)

# Plotting the Beta Values -- Or the highest word probabilities for the topics.  
grimms_beta %>%
    group_by(topic) %>%
    top_n(10, beta) %>%
    ungroup() %>%
    mutate(topic = paste0("Topic ", topic),
           term = reorder_within(term, beta, topic)) %>%
    ggplot(aes(term, beta, fill = as.factor(topic))) +
    geom_col(alpha = 0.8, show.legend = FALSE) +
    facet_wrap(~ topic, ncol = 5, scales = "free_y") +
    coord_flip() +
    scale_x_reordered() +
    labs(x = NULL, y = expression(beta),
         title = "Highest word probabilities for each topic")

```

And, lets also explore how documents are associated to each topic. For this step, we want pretty divisive topics whereby there are a few documents that have a high probability of being associated with a topic, and many documents with a low probability of being associated with a topic. 

```{r message=FALSE}
# Using the tidy package to quickly and easily extract data from the model. 
grimms_gamma <- tidy(topic_model, matrix = "gamma",                    
                 document_names = rownames(grimms_dfm))

# Plotting the Gamma Values -- Or the distribution of documents within each topic. 
  # Probabilits close to zero are removed here to help with clearer visualization. 
grimms_gamma %>%
  filter(gamma > 0.1) %>%
  ggplot(aes(gamma, fill = as.factor(topic))) +
  geom_histogram(alpha = 0.8, show.legend = FALSE) +
  facet_wrap(~ topic, ncol = 5) +
  labs(title = "Distribution of document probabilities for each topic",
       y = "Number of stories", x = expression(gamma))
```

## Conclusion
This is just an initial introduction to some major text processing topics, but it is by no means exhaustive. Every type of data and every type of situation requires some special fanagling in order to make text analysis run smoothly and in order to make sure the models are reliable. I hope this kicks off your explorations into text analysis and that you will continue learning in the future. 

#### Thank You! 